{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import bz2\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.19.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gmsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# importing all necessary modules \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "train_labels, train_texts = get_labels_and_texts('train.ft.txt.bz2')\n",
    "test_labels, test_texts = get_labels_and_texts('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ALPHANUM = re.compile(r'[\\W]')\n",
    "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "def normalize_texts(texts):\n",
    "    normalized_texts = []\n",
    "    for text in texts:\n",
    "        lower = text.lower()\n",
    "        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
    "        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
    "        normalized_texts.append(no_non_ascii)\n",
    "    return normalized_texts\n",
    "        \n",
    "train_texts = normalize_texts(train_texts)\n",
    "test_texts = normalize_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_texts)\n",
    "tokenized_val = tokenizer.texts_to_sequences(val_texts)\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_texts, val_texts, test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "X_val = sequence.pad_sequences(tokenized_val, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#GloVes Load\n",
    "EMBEDDING_DIM = 200\n",
    "Glove = 'glove.6B.200d.txt'\n",
    "\n",
    "glove2word2vec(glove_input_file=Glove, word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n",
    "\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(20000, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
    "\n",
    "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "    if embeddings_matrix.shape[0] == 20000:\n",
    "        break \n",
    "        \n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    sequences = layers.Input(shape=(maxlen,))\n",
    "    embedded = layers.Embedding(input_dim =embeddings_matrix.shape[0],output_dim = embeddings_matrix.shape[1],weights=[embeddings_matrix],trainable=False, input_length = maxlen)(sequences)\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(3)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(5)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    opt = optimizers.Adam(lr=0.0005)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2880000 samples, validate on 720000 samples\n",
      "Epoch 1/10\n",
      "2880000/2880000 [==============================] - 208s 72us/sample - loss: 0.3473 - binary_accuracy: 0.8388 - val_loss: 0.2870 - val_binary_accuracy: 0.8762: 3:38 - loss: 0.6215 - binary_accura - ETA: 3:34 - loss: 0.6162 - binary_acc - ETA: 3:30 - loss: - ETA: 3:21 - loss: 0.5923 - binary_accura - ETA: 2:11 - loss: 0.4505 - binary_accuracy: - ETA: 2:11 - loss: 0.4495 - binary_ - ETA: 2:02 - loss: 0.4359 - binary_accuracy: 0 - ETA: 2:01 - loss: 0.4354 - bi - ETA: 2:00 - l - ETA: 1:50 - loss: 0.4201 - binary_accurac - ETA: 1:49 - loss: 0.4194 - binary_ac - ETA: 1:48 - loss: 0.4183 - binary_accuracy - ETA: 1:48 - loss: 0.4177 - binary_accur - ETA: 1:47 - loss: 0.4168 - binary_accur - ETA: 1:46 - loss: 0.4160 - binar - ETA: 1:45 - loss: 0.4145 - binary_acc - ETA: 1:44 - loss: 0.4135 - binary_accuracy:  - ETA: 1:44 - loss: 0.4131 - binary_accuracy: 0. - ETA: 1:43 - loss: 0.4128 - binary_a - ETA: 1:42 - loss: 0.4116 - binary_accur - ETA: 1:42 - loss: 0.4108 - binary_accuracy: 0.7 - ETA: 1:42 - loss: 0.4106 -  - ETA: 1:40 - loss: 0.40 - ETA: 1:38 - loss: 0.4071 - binary_accuracy: 0. - ETA: 1:38 - loss - ETA: 1:36 - loss: 0.4045 - binary_accuracy:  - ETA: 1:35 - loss: 0.4042 - bina - ETA: 1:34 - loss: 0.4030 - binary_accur - ETA: 1:33 - loss: 0.4023 - binary_accuracy: 0.8 - ETA: 1:33 - loss: 0.4021 - binary_accuracy: 0. - ETA: 1:33 - loss: 0.4 - ETA: 1:31 - loss: 0.4000 - binary_accuracy: 0. - ETA: 1:31 - loss: 0.3998 - binary_accuracy: 0.80 - ETA: 1:31 - loss: 0.3996 - binary_accuracy:  - ETA: 1:30 - loss: 0.3992 - binary_accuracy: 0  - ETA: 1:23 - loss: 0. - ETA: 1:21 - loss: 0.3913 - binary_accuracy: 0 - ETA: 1:21 - loss: 0 - ETA: 1:08 - loss: 0.3814 - binary_accuracy: 0.8 - ETA: 1:08 - loss: 0.3813 - binary_accuracy: 0.81 - ETA: 1:08 - loss: 0.3812 - binary_accuracy - ETA: 1:07 - - ETA: 1:05 - loss: 0.3791  - ETA: 47s - loss - ETA: 46s - loss: 0.3680 - binary_ac - ETA: 46s - loss: 0.3677 - ETA: 45s - lo - ETA: 39s - loss: 0.36 - ETA: 38s - loss: 0.36 - ETA: 37s - loss: 0.3630 - binary_accuracy - ETA: 37s - loss: 0.3629 - - ETA: 36 - ETA: 34s - loss: 0.3619 - binary_accuracy: 0.82 - ETA: 34s - loss: 0.3619 - binary_accuracy:  - ETA: 34s  - ETA: 33s - loss: 0.3611 - binary_accuracy: 0.83 - ETA: 33s - loss: 0.3611 - binary_accuracy: 0.83 - ETA: 33s - loss: 0.3611 - binary_accu - ETA: 32s - loss: 0.3609 - binary_ac - ETA: 32s - loss: 0.3607 - binary_ - ETA: 31s - loss: 0.36 - ETA: 30s - loss: 0.3600 - binary_accuracy - ETA: 30s - loss: 0.3599 - binar - ETA: 30s - loss: 0.3596 - binary_accuracy: 0.83 - ETA: 30s - loss: 0.3596 - binary_accuracy: 0.83 - ETA: 29s - loss: 0.3596 - bin - ETA: 29s - loss: 0.3593 - binary_accuracy: 0.83 - ETA: 29s - loss: 0.3592 - bin - ETA: 28s - loss - ETA: 27s - loss: 0.3584 - binary_accuracy:  - ETA: 27s - loss: 0.3583 - bin - ETA: 26s - loss: 0.3580 - binar - ETA: 26s - loss: 0.3578 - b - ETA:  - ETA: 23s - loss: 0.3568 - binary_accu - ETA: 21s - loss: 0.3559 - - ETA: 19s - loss: 0.3549 - binary_accuracy: 0. - ETA: 19s - loss:  - ETA: 18s - loss: 0.3544 - binary_accuracy: 0.83 - E - ETA: 16s - loss: 0.3537 - binary_ac - ETA - ETA: 11s - loss: 0.3515 - - ETA: 10s - loss: 0.3512 - binary_ac - ETA: 9s - loss: 0.3510 - binary_accuracy: - ETA: 9s - loss: 0.3508 - binary_ac - ETA: 8s - loss: 0.3505 - binary_accu - ETA: 7s - loss: 0.350 - ETA: 2s - loss: 0.3483 - binary_ - ETA: 1s - loss: 0.3479 - binary_accuracy:  - ETA: 1s - loss: 0.3477 - binar\n",
      "Epoch 2/10\n",
      "2880000/2880000 [==============================] - 217s 75us/sample - loss: 0.2664 - binary_accuracy: 0.8872 - val_loss: 0.2577 - val_binary_accuracy: 0.8916oss: 0.2734 - binary_accura - ETA: 2:59 - loss: 0.2759  - ETA: 2:57 - loss: 0.2754 - binary_accuracy:  - ETA: 2:56 - loss: 0.275 - ETA: 2:55 - loss: 0.2759 - binary_accuracy: 0.88 - ETA: 2:55 - loss: 0.2758 - binary_accuracy: 0. - ETA: 2:54 - loss: 0.2755 - binary_accuracy: - ETA: 2:54 - loss: 0.2753 - binary_accuracy: 0.8 - ETA: 2:54 - loss: 0.2752 - binary_accuracy:  - ETA: 2:53 - loss: 0.2749 - binary_accura - ETA: 2:53 - loss: 0.2750 - bina - ETA: 2:51 - loss: 0.2753 - binary_accuracy - ETA: 2:51 - loss: 0.2747 - binary_accuracy: 0.882 - ETA: 2:51 - loss: 0.2747 - binary_accuracy:  - ETA: 2:51 - loss: 0.2749 - binary_accuracy: 0.8 - ETA: 2:50 - loss: 0.2748 - b - ETA: 2:49 - loss: 0.2747 - bi - ETA: 2:47 - loss: 0.2748 - binary_accuracy: 0.8 - ETA: 2:47 -  - ETA: 2:46 - loss: 0.2749 - bina - ETA: 2:44 - loss: 0.2746 - binary_ac - ETA: 2:43 - loss: 0.2744 - binary_accuracy: 0.8 - ETA: 2:43 - loss: 0 - E - ETA:  - ETA: 2:31 - loss: 0. - ETA: 2:09 - loss: 0.2730 - binary_ac - ETA: 2:08 - loss: 0.2729 - binary_accura - ETA: 2:08 -  - ETA: 2:05 - loss: 0.2727 - bina - ETA: 2:04 - loss: 0.2725 - binary_accuracy: 0. - ETA: 2:04 - loss: 0.2725 - binary_accuracy:  - ETA: 2:03 - loss: 0.2725 - binary_accuracy: 0. - ETA: 2:03 - loss: 0.2725 - binary_ - ETA: 2:02 - loss: 0.2723 -  - ETA: 2:01 - loss: 0.2722 - binary_ - ETA: 2:00 - loss: 0.2722 - binar - ETA: 6s - loss: 0.2668 - b - ETA: 5s - loss: 0 - \n",
      "Epoch 3/10\n",
      "2880000/2880000 [==============================] - 219s 76us/sample - loss: 0.2494 - binary_accuracy: 0.8958 - val_loss: 0.2524 - val_binary_accuracy: 0.8942\n",
      "Epoch 4/10\n",
      "2880000/2880000 [==============================] - 218s 76us/sample - loss: 0.2399 - binary_accuracy: 0.9004 - val_loss: 0.2446 - val_binary_accuracy: 0.8983\n",
      "Epoch 5/10\n",
      "2880000/2880000 [==============================] - 219s 76us/sample - loss: 0.2331 - binary_accuracy: 0.9040 - val_loss: 0.2492 - val_binary_accuracy: 0.8951A: 0s - loss: 0.2331 - binary_a\n",
      "Epoch 6/10\n",
      "2880000/2880000 [==============================] - 218s 76us/sample - loss: 0.2281 - binary_accuracy: 0.9063 - val_loss: 0.2478 - val_binary_accuracy: 0.8970 - lo\n",
      "Epoch 7/10\n",
      "2880000/2880000 [==============================] - 218s 76us/sample - loss: 0.2241 - binary_accuracy: 0.9082 - val_loss: 0.2362 - val_binary_accuracy: 0.9027\n",
      "Epoch 8/10\n",
      "2880000/2880000 [==============================] - 219s 76us/sample - loss: 0.2209 - binary_accuracy: 0.9097 - val_loss: 0.2494 - val_binary_accuracy: 0.8958\n",
      "Epoch 9/10\n",
      "2880000/2880000 [==============================] - 218s 76us/sample - loss: 0.2183 - binary_accuracy: 0.9110 - val_loss: 0.2334 - val_binary_accuracy: 0.90340.2183 - binary_accura\n",
      "Epoch 10/10\n",
      "2880000/2880000 [==============================] - 219s 76us/sample - loss: 0.2160 - binary_accuracy: 0.9121 - val_loss: 0.2460 - val_binary_accuracy: 0.8979\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    train_labels, \n",
    "    batch_size=1024,\n",
    "    epochs=10,\n",
    "     validation_data=(X_val, val_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('glove_model_200_0005.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "embeddings_matrix = pd.read_csv('skipgram_embeddings_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2880000 samples, validate on 720000 samples\n",
      "Epoch 1/5\n",
      "2880000/2880000 [==============================] - 3887s 1ms/sample - loss: 0.2488 - binary_accuracy: 0.8960 - val_loss: 0.2238 - val_binary_accuracy: 0.9088\n",
      "Epoch 2/5\n",
      "2880000/2880000 [==============================] - 3761s 1ms/sample - loss: 0.2046 - binary_accuracy: 0.9181 - val_loss: 0.2014 - val_binary_accuracy: 0.9192\n",
      "Epoch 3/5\n",
      "2880000/2880000 [==============================] - 3688s 1ms/sample - loss: 0.1927 - binary_accuracy: 0.9237 - val_loss: 0.2098 - val_binary_accuracy: 0.9163\n",
      "Epoch 4/5\n",
      "2880000/2880000 [==============================] - 3691s 1ms/sample - loss: 0.1858 - binary_accuracy: 0.9268 - val_loss: 0.1982 - val_binary_accuracy: 0.9206\n",
      "Epoch 5/5\n",
      "2880000/2880000 [==============================] - 3689s 1ms/sample - loss: 0.1810 - binary_accuracy: 0.9290 - val_loss: 0.1948 - val_binary_accuracy: 0.9226\n"
     ]
    }
   ],
   "source": [
    "history_skipgram = model_skipgram.fit(\n",
    "    X_train, \n",
    "    train_labels, \n",
    "    batch_size=1024,\n",
    "    epochs=5,\n",
    "    validation_data=(X_val, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram.save('skipgram_model_0005.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000/400000 [==============================] - 112s 279us/sample - loss: 0.1970 - binary_accuracy: 0.9223: - ETA: 1:45 - loss: 0.2039 - binary_accuracy: 0.9 - ETA: 1:45 - ETA:  - ETA: 1:33 - loss: 0.2039 - binary_accuracy:  - ETA: 1:33 - loss - ETA: 1:30 - loss: 0.2034  - ETA: 1:28 - loss: 0.2031 - binary_accuracy: - ETA: 1:28 - loss: 0.2028 - binary_accuracy:  - ETA: 1: - ETA: 1 - ETA: 1:21 - loss: 0.2031 - binary_accuracy - ETA: 1:10 - loss: 0.2015 - bin - ETA: 1:09 - loss: 0.2013 - binary_accuracy - ETA: 1:08 - loss: 0.2013 - binary_accuracy: 0.92 - ETA: 1:08 - loss: 0.2012 - binary_ - ETA: 1:07 - loss: 0.2013 - binary_accuracy: 0. - ETA: 1:06 - loss: 0.2013 - binary_ - ETA: 1:05 - loss: 0.2014 - binary_accuracy: 0.92 - ETA: 1:05 - loss: 0.2014 - binary_accurac - ETA: 1:04 - loss: 0.2014 - binary_accuracy: - ETA: 1:04 - loss: 0.2015 - binary_accuracy - ETA: 1:03 - loss: 0.2014 - binary_accuracy: 0.92 - ETA: 1:03 - loss: 0.2014 - binary - ETA: 1:02 - loss: 0.2012 - binary_ -  - ETA: 56s - loss: 0.2006 - binary_accuracy - ETA: 56s - loss: 0.2006 - binary_ac - ETA: 55s - loss: 0.20 - ETA:  - ETA: 51s - loss: 0. - ETA: 50s - loss: 0.2007 - binary_accura - ETA: 49s - loss: 0.2006 - ETA: 48s - loss: 0.2003 - - ETA: 47s - loss: 0.2004 - ETA: 47s - lo - ETA: 45s - loss: 0.2004 - binary_accura - ETA: 43s - loss: 0.1999 - binary_accuracy: 0. - ETA: 43s - loss: 0.1999 - binary_accuracy - ETA: 43s - loss: 0.2001 - ETA: 25s - loss: 0.1988 - bin - ETA:  - ETA: 23s - loss: 0.1987 - ETA: 22s - loss: 0.1987 - b - ETA: 21s - loss: 0.1986 - ETA: 20s - loss: 0.1986 - binary_accura - ETA: 20s - loss: 0.1986 - binary_accuracy: 0. - ETA:  - ETA: 16s - loss: 0.1981 - bin - ETA: 15s - loss: 0.1981 - binar - ETA: 9s - loss: 0.1978 - binary_accuracy:  - ETA: 8s - loss: 0.1979 - binary_accuracy: 0.9 - ETA: 8s - loss: 0.1979 - binary_accuracy: 0.92 - ETA: 8s - loss: 0.1979 - binary_ac - ETA: 7s - loss: 0.1978 - binary_accuracy: - ETA: 7s - loss: 0.1977 - binary - ETA: 5s - loss: 0.1975 - binary_accuracy: 0. - ETA: 5s - loss: 0.1975 - binary_ac - ETA: 0s - loss: 0.1972 - binary_ac\n",
      "results for skipgram \n",
      "test loss, test acc: [0.19700973466038704, 0.9223]\n"
     ]
    }
   ],
   "source": [
    "results_skipgram = model_skipgram.evaluate(X_test, test_labels, batch_size=128)\n",
    "print(\"results for skipgram \")\n",
    "print(\"test loss, test acc:\", results_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
